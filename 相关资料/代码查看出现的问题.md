![1611122831018](C:\Users\13018\AppData\Roaming\Typora\typora-user-images\1611122831018.png)

 batch normalization就是强行将数据拉回到均值为0，方差为1的正太分布上，这样不仅数据分布一致，而且避免发生梯度消失。 

 ![这里写图片描述](https://img-blog.csdn.net/20180714175844131?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXhpYW8yMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 

~~~python
layer = input (None,1280,1)

add_resnet_layer(layer)
​	layer = add_conv_weight(layer,filter-32*32,num-12,subsample-1)
		layer = Conv1D_1(num-12,filter-32*32,stride=1,) (None,1280,12)
​ 	layer = _bn_relu(layer,dropout=0)	
		layer = BatchNormalization_1()(layer) (None,1280,12)
    	layer = Activation_1(config.conv_activation)(layer) (None, 1280, 12)
	# index=[0,1,2,3,4,5,6,7]
 	# subsample=[1,2,1,2,1,2,1,2]
    # num_filter=[12,12,24,24,48,48,96,96]
	# zero_pad = [F,F,T,F,T,F,T,F]
​	layer = resnet_block(layer,num-12,subsample-1,index-0)
		shortcut = MaxPooling1D_1(pool_size=subsample_length=1)(layer)  (None,1280,12) # 代码运行这里，实际在最后才添加才add
    	layer = add_conv_weight(layer,filter-32*32,num-12,subsamlpe=1)
        	layer = Conv1D_2(num-12,filter-32*32,stride=1,) (None,1280,12)
        layer = _bn_relu(layer,dropout=0.5)
        	layer = BatchNormalization_2()(layer) (None,1280,12)
    		layer = Activation_2(config.conv_activation)(layer) (None,1280,12)
            layer = Dropout_1(dropout=0.5)(layer) (None,1280,12)
        layer = add_conv_weight(layer,filter-32*32,num-12,subsample=1)
        	layer = Conv1D_3(num-12,filter-32*32,stride=1,)  (None,1280,12)
        layer = Add_1()([shortcut,9 layer])  (None,1280,12) # 将MaxPooling1D_1的输出结果(None,1280,12)和Conv1D_3的输出结果(None,1280,12)的结果相加  Activation_2,Dropout_1,MaxPooling1D_1,Conv1D_3,Add_1()
   
​   layer = resnet_block(layer,num-12,subsample-2,index-1)
		shortcut = MaxPooling1D_2(pool_size=subsample_length=2)(layer)	(None,640,12)
    	layer = _bn_relu(layer,dropout=0)
        	layer = BatchNormalization_3()(layer) (None, 1280, 12)
    		layer = Activation_3(config.conv_activation)(layer) (None, 1280, 12)
        layer = add_conv_weight(layer,filter-32*32,num-12,subsample=2)
        	layer = Conv1D_4(num-12,filter-32*32,stride=2,) (None, 1280, 12)
        layer = _bn_relu(layer,dropout=0.5)
        	layer = BatchNormalization_4()(layer) (None, 640, 12)   
    		layer = Activation_4(config.conv_activation)(layer) (None, 640, 12)   
            layer = Dropout_2(dropout=0.5)(layer) (None, 640, 12)   
        layer = add_conv_weight(layer,filter-32*32,num-12,subsample=2) 
        	layer = Conv1D_5(num-12,filter-32*32,stride=2,) (None, 640, 12)   
        layer = Add_2()([shortcut, layer]) (None, 640, 12)   # Activation_4,Dropout_2,MaxPooling1D_2,Conv1D_5,Add_2()
            
​   layer = resnet_block(layer,num-24,subsample-1,index-2)
		shortcut = MaxPooling1D_3(pool_size=subsample_length=1)(layer) (None,640,12)
    	shortcut = Lambda_1(zeropad, output_shape=zeropad_output_shape)(shortcut) (None,640,24)
        layer = _bn_relu(layer,dropout=0)
        	layer = BatchNormalization_5()(layer) (None, 640, 12)
    		layer = Activation_5(config.conv_activation)(layer) (None, 640, 12)
        layer = add_conv_weight(layer,filter-32*32,num-24,subsample=1)
        	layer = Conv1D_6(num-24,filter-32*32,stride=1,)  (None, 640, 24)
		layer = _bn_relu(layer,dropout=0.5)
        	layer = BatchNormalization_6()(layer)  (None, 640, 24)
    		layer = Activation_6(config.conv_activation)(layer)  (None, 640, 24)
            layer = Dropout_3(dropout=0.5)(layer) (None, 640, 24)
        layer = add_conv_weight(layer,filter-32*32,num-24,subsample=1)
        	layer = Conv1D_7(num-24,filter-32*32,stride=1,) (None, 640, 24)
        layer = Add_3()([shortcut, layer]) (None, 640, 24) # activation_6, MaxPooling1D_3,Dropout_3,Lambda_1,Conv1D_7,Add_3()
        
​   layer = resnet_block(layer,num-24,subsample-2,index-3)
		shortcut = MaxPooling1D_4(pool_size=subsample_length=2)(layer) (None,320,24)
    	layer = _bn_relu(layer,dropout=0)
        	layer = BatchNormalization_7()(layer) (None, 640, 24)
    		layer = Activation_7(config.conv_activation)(layer) (None, 640, 24)
        layer = add_conv_weight(layer,filter-32*32,num-24,subsample=2)
        	layer = Conv1D_8(num-24,filter-32*32,stride=2,) (None, 320, 24)
		layer = _bn_relu(layer,dropout=0.5)
        	layer = BatchNormalization_8()(layer) (None, 320, 24)
    		layer = Activation_8(config.conv_activation)(layer) (None, 320, 24)
            layer = Dropout_4(dropout=0.5)(layer) (None, 320, 24)
        layer = add_conv_weight(layer,filter-32*32,num-24,subsample=2)
        	layer = Conv1D_9(num-24,filter-32*32,stride=2,) (None, 320, 24)
        layer = Add_4()([shortcut, layer]) (None, 320, 24) # Activation_8,Dropout_4,MaxPooling1D_4,Conv1D_9,Add_4
        
​   layer = resnet_block(layer,num-48,subsample-1,index-4)
		shortcut = MaxPooling1D_5(pool_size=subsample_length=1)(layer) (None,320, 24) 
    	shortcut = Lambda_2(zeropad, output_shape=zeropad_output_shape)(shortcut) (None, 320, 48)
        layer = _bn_relu(layer,dropout=0)
        	layer = BatchNormalization_9()(layer) (None, 320, 24) 
    		layer = Activation_9(config.conv_activation)(layer)	(None, 320, 24) 
    	layer = add_conv_weight(layer,filter-32*32,num-48,subsample=1)
        	layer = Conv1D_10(num-48,filter-32*32,stride=1,) (None, 320, 48)
        layer = _bn_relu(layer,dropout=0.5)
        	layer = BatchNormalization_10()(layer) (None, 320, 48)
    		layer = Activation_10(config.conv_activation)(layer) (None, 320, 48)
            layer = Dropout_5(dropout=0.5)(layer) (None, 320, 48)        
        layer = add_conv_weight(layer,filter-32*32,num-48,subsample=1)
        	layer = Conv1D_11(num-48,filter-32*32,stride=1,) (None, 320, 48)
        layer = Add_5()([shortcut, layer]) (None, 320, 48) #activation_10,MaxPooling1D_5,Dropout_5,Lambda_2,Conv1D_11,Add_5
        
​   layer = resnet_block(layer,num-48,subsample-2,index-5)
		shortcut = MaxPooling1D_6(pool_size=subsample_length=2)(layer)  (None,160,48)
        layer = _bn_relu(layer,dropout=0)
        	layer = BatchNormalization_11()(layer) (None, 320, 48)
    		layer = Activation_11(config.conv_activation)(layer) (None, 320, 48)
        layer = add_conv_weight(layer,filter-32*32,num-48,subsample=2)
        	layer = Conv1D_12(num-48,filter-32*32,stride=2,) (None, 160, 48)
        layer = _bn_relu(layer,dropout=0.5)
        	layer = BatchNormalization_12()(layer) (None, 160, 48)
    		layer = Activation_12(config.conv_activation)(layer) (None, 160, 48)
            layer = Dropout_6(dropout=0.5)(layer) (None, 160, 48)
        layer = add_conv_weight(layer,filter-32*32,num-48,subsample=2)
        	layer = Conv1D_13(num-48,filter-32*32,stride=2,) (None, 160, 48)
        layer = Add_6()([shortcut, layer]) (None, 160, 48) #Activation_12,Dropout_6,MaxPooling1D_6,Conv1D_13,Add_6   
        
​   layer = resnet_block(layer,num-96,subsample-1,index-6)
		shortcut = MaxPooling1D_7(pool_size=subsample_length=1)(layer)  (None,160,48)
    	shortcut = Lambda_3(zeropad, output_shape=zeropad_output_shape)(shortcut) (None, 160, 96)
        layer = _bn_relu(layer,dropout=0)
        	layer = BatchNormalization_13()(layer) (None, 160, 48)
    		layer = Activation_13(config.conv_activation)(layer) (None, 160, 48)
        layer = add_conv_weight(layer,filter-32*32,num-96,subsample=1)
        	layer = Conv1D_14(num-96,filter-32*32,stride=1,) (None, 160, 96)
        layer = _bn_relu(layer,dropout=0.5)
        	layer = BatchNormalization_14()(layer) (None, 160, 96)
    		layer = Activation_14(config.conv_activation)(layer) (None, 160, 96)
            layer = Dropout_7(dropout=0.5)(layer) (None, 160, 96)
        layer = add_conv_weight(layer,filter-32*32,num-96,subsample=1)
        	layer = Conv1D_15(num-96,filter-32*32,stride=1,) (None, 160, 96)
        layer = Add_7()([shortcut, layer]) (None, 160, 96) # Activation_14,MaxPooling1D_7,Dropout_7,Lambda_3,Conv1D_15,Add_7()
        
​   layer = resnet_block(layer,num-96,subsample-2,index-7)
		shortcut = MaxPooling1D_8(pool_size=subsample_length=2)(layer)(None,80,96)
    	layer = _bn_relu(layer,dropout=0)
        	layer = BatchNormalization_15()(layer) (None, 160, 96)
    		layer = Activation_15(config.conv_activation)(layer) (None, 160, 96)
        layer = add_conv_weight(layer,filter-32*32,num-96,subsample=2)
        	layer = Conv1D_16(num-96,filter-32*32,stride=2,) (None, 80, 96)
        layer = _bn_relu(layer,dropout=0.5)
        	layer = BatchNormalization_16()(layer) (None, 80, 96)
    		layer = Activation_16(config.conv_activation)(layer) (None, 80, 96)
            layer = Dropout_8(dropout=0.5)(layer) (None, 80, 96)
        layer = add_conv_weight(layer,filter-32*32,num-96,subsample=2)
        	layer = Conv1D_17(num-96,filter-32*32,stride=2,)  (None, 80, 96)
        layer = Add_8()([shortcut, layer]) (None, 80, 96) # Activation_16,Dropout_8,MaxPooling1D_8,Conv1D_17,Add_8()
        
​   layer = _bn_relu(layer)	
		layer = BatchNormalization_17()(layer) (None, 80, 96)
    	layer = Activation_17(config.conv_activation)(layer) (None, 80, 96)
​  	output = add_output_layer(layer)
    	layer = GlobalAveragePooling1D_1()(layer) (None, 96) # 96个特征图做平均池化
    	layer = Dense(config.num_categories)(layer) (None, 5) 
        Activation_18('softmax')(layer) (None, 5) 
​   add_compile(model,config)
    	optimizer = SGD(lr=config.lr_schedule(0), momentum=0.9)
    	model.compile(loss='categorical_crossentropy',
                  optimizer=optimizer,
                  metrics=['categorical_accuracy'])
~~~





​	